import torch
import torch.nn as nn
from torch.autograd import Variable
import pandas as pd
import torch.utils.data
import matplotlib.pyplot as plt
import numpy as np
from sklearn import tree
from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, precision_recall_curve, f1_score, auc, accuracy_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
import seaborn as sns
import bottleneck

torch.manual_seed(0)
# Cargamos los datos
Dataset = np.genfromtxt('Dataset.txt',delimiter=',', dtype=np.int32)
data_input = Dataset[:, :-1]
data_output = Dataset[:, -1]
# data_output = label_binarize(data_output, classes=[0,1,2,3,4])

# for index, value in enumerate(data_output):
#     if value==2 or value==3 or value==4:
#         data_output[index] = 0
#     elif value==1:
#         data_output[index] = 1
        
features_train, features_test, labels_train, labels_test = train_test_split(data_input, data_output, shuffle=True, random_state=32)

###################################
################TREE###############
###################################

classifier_tree = tree.DecisionTreeClassifier(random_state=32)
classifier_tree.fit(features_train, labels_train)
prediction_tree = classifier_tree.predict(features_test)
accuracy_tree = 100 * accuracy_score(labels_test, prediction_tree)
print("Modelo arbol de decisiones:")
print('acc (%): ', round(accuracy_tree, 2))

print("Matriz de confusion: \n", confusion_matrix(labels_test, prediction_tree),"\n")
print("Precision: ",precision_score(labels_test, prediction_tree, average='micro')) 
print("Exhaustividad:  ",recall_score(labels_test, prediction_tree, average='micro'),"\n")      

# # precision recall curve
# precision = dict()
# recall = dict()
# for i in range(1):
#     precision[i], recall[i], _ = precision_recall_curve(labels_test[:],
#                                                         prediction_tree[:])
#     plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))
        
# plt.xlabel("recall")
# plt.ylabel("precision")
# plt.legend(loc="best")
# plt.title("precision vs. recall curve")


# # roc curve
# fpr = dict()
# tpr = dict()

# for i in range(1):
#     fpr[i], tpr[i], _ = roc_curve(labels_test[:],
#                                   prediction_tree[:])
#     plt.plot(fpr[i], tpr[i], lw=2, label='class {}'.format(i))

# plt.xlabel("false positive rate")
# plt.ylabel("true positive rate")
# plt.legend(loc="best")
# plt.title("ROC curve")


# false_positive_rate, true_positive_rate, threshold = roc_curve(labels_test, prediction_tree)
# plt.figure(3)
# plt.plot(false_positive_rate, true_positive_rate, label="TreeDecision()")

# lr_auc = roc_auc_score(labels_test, prediction_tree)    
# print("classifier_tree", ': ROC AUC=%.3f' % (lr_auc))     
# lr_precision, lr_recall, _ = precision_recall_curve(labels_test, prediction_tree)
# lr_f1  = f1_score(labels_test, prediction_tree, average='micro')
# lr_auc = auc(lr_recall, lr_precision)
# print("classifier_tree", ': f1=%.3f' % (lr_f1))
# print("classifier_tree", ': auc=%.3f' % (lr_auc))
# no_skill = len(labels_test[labels_test==1]) / len(labels_test)    
# plt.figure(4)    
# plt.plot(lr_recall, lr_precision, marker='.', label="TreeDecision()")

orden = -bottleneck.partition(-classifier_tree.feature_importances_, 20)[:20]
# feature_importances = DataFrame(classifier_tree.feature_importances_, index = x_train.columns, columns['importance']).sort_values('importance', ascending=False)
for i in sorted(orden, reverse=True):
    print(np.where(classifier_tree.feature_importances_ == i))
    print(i)    

##################################
###############DL#################
##################################
class DeepNeuralNetwork(nn.Module):
    def __init__(self, inputSize, mod, hidden1Size, hidden2Size, hidden3Size, hidden4Size, hidden5Size, hidden6Size, hidden7Size, numClasses):
        super(DeepNeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(inputSize, hidden1Size)
        if mod == "Sigmoid":
            self.nodos1 = nn.Sigmoid()
            self.nodos2 = nn.Sigmoid()
        elif mod == "PRelU":
            self.nodos1 = nn.PReLU()
            self.nodos2 = nn.PReLU()
            self.nodos3 = nn.PReLU()
            self.nodos4 = nn.PReLU()
            self.nodos5 = nn.PReLU()
            self.nodos6 = nn.PReLU()
            self.nodos7 = nn.PReLU()
        elif mod == "Tanh":
            self.nodos1 = nn.Tanh()  
            self.nodos2 = nn.Tanh()
        self.fc2 = nn.Linear(hidden1Size, hidden2Size)
        self.fc3 = nn.Linear(hidden2Size, hidden3Size)
        self.fc4 = nn.Linear(hidden3Size, hidden4Size)
        self.fc5 = nn.Linear(hidden4Size, hidden5Size)
        self.fc6 = nn.Linear(hidden5Size, hidden6Size)
        self.fc7 = nn.Linear(hidden6Size, hidden7Size)
        self.fc8 = nn.Linear(hidden7Size, numClasses)
        self.logsm1 = nn.LogSoftmax(dim=1)

    def forward(self, x):
        out = self.fc1(x)
        out = self.nodos1(out)
        out = self.fc2(out)
        out = self.nodos2(out)
        out = self.fc3(out)
        out = self.nodos3(out)
        out = self.fc4(out)
        out = self.nodos4(out)
        out = self.fc5(out)
        out = self.nodos5(out)
        out = self.fc6(out)
        out = self.nodos6(out)
        out = self.fc7(out)
        out = self.nodos7(out)
        out = self.fc8(out)
        out = self.logsm1(out)
        return out

# Hiperparametros
inputSize = data_input.shape[1]
print(data_input.shape[1])
hidden1Size = 470
hidden2Size = 470
hidden3Size = 470
hidden4Size = 470
hidden5Size = 470
hidden6Size = 470
hidden7Size = 470
numClasses = 5
numEpoch = 1000
batchsize = 100
lr=0.0009

# model_Sigmoid = DeepNeuralNetwork(inputSize, "Sigmoid", hidden1Size, hidden2Size, hidden3Size, hidden4Size numClasses)
# model_Tanh = DeepNeuralNetwork(inputSize, "Tanh", hidden1Size, hidden2Size, hidden3Size, hidden4Size numClasses)
model_RelU = DeepNeuralNetwork(inputSize, "PRelU", hidden1Size, hidden2Size, hidden3Size, hidden4Size, hidden5Size, hidden6Size, hidden7Size, numClasses)

criterion = nn.NLLLoss()
# optimizer_Sigmoid = torch.optim.Adam(model_Sigmoid.parameters(), lr=lr)
# optimizer_Tanh = torch.optim.Adam(model_Tanh.parameters(), lr=lr)
optimizer_RelU = torch.optim.Adam(model_RelU.parameters(), lr=lr)

def train(epochs, model, batchsize, optimizer):
    plot_loss=np.zeros(epochs)
    accuracy=np.zeros(epochs)
    x_train = Variable(torch.from_numpy(features_train)).float()
    y_train = Variable(torch.from_numpy(labels_train)).long()
    model.train()
    for epoch in range(epochs):
        i=0
        while i <= (len(x_train)-batchsize):
            optimizer.zero_grad()
            y_pred = model(x_train[i:i+batchsize-1,:])
            loss = criterion(y_pred, y_train[i:i+batchsize-1])
            plot_loss[epoch]=loss.item()
            # pred = torch.max(y_pred, 1)[1].eq(y_train[i:i+batchsize-1]).sum()
            loss.backward()
            optimizer.step()
            i+=batchsize
        optimizer.zero_grad()
        y_pred = model(x_train[i:-1,:])
        loss = criterion(y_pred, y_train[i:-1])
        plot_loss[epoch]=loss.item()
        # pred = torch.max(y_pred, 1)[1].eq(y_train[i:-1]).sum()
        loss.backward()
        optimizer.step()        
        
        model.eval()
        x_test = Variable(torch.from_numpy(features_test)).float()
        y_test = Variable(torch.from_numpy(labels_test)).long()
        y_pred = model(x_test)
        loss = criterion(y_pred, y_test)
        accuracy[epoch] = 100*torch.max(y_pred, 1)[1].eq(y_test).sum()/len(x_test)
        print(epoch)
    plt.figure(1)
    plt.plot(plot_loss, marker='.', label=str(model.nodos1))
    plt.legend()
    plt.ylabel("Loss")
    plt.xlabel("Epoch")
    print('acc (%): ', round(accuracy[epochs-1], 2))
    print('Mac acc (%): ', round(max(accuracy), 2), 'Epoch: ', np.where(accuracy == max(accuracy)))    
    plt.figure(2)
    plt.plot(accuracy, marker='.', label=str(model.nodos1))
    plt.legend()
    plt.ylabel("Accuracy")
    plt.xlabel("Epoch")
# print("Modelo Sigmoid")
# train(numEpoch, model_Sigmoid, batchsize, optimizer_Sigmoid)
print("Modelo PRelU")
train(numEpoch, model_RelU, batchsize, optimizer_RelU)
# print("Modelo Tanh")
# train(numEpoch, model_Tanh, batchsize, optimizer_Tanh)

def evaluarbinary(model):
    model.eval()
    pred = model(torch.from_numpy(features_test).float())
    pred = torch.max(pred,1)[1]
    pred = pred.data.numpy()
    MC = confusion_matrix(labels_test, pred).ravel()
    print("Matriz de confusion:", MC)
    listpath = ["Benign", "Malware"]
    df_MC = pd.DataFrame(MC.reshape(2,2), index=listpath, columns=listpath)
    plt.figure(3)
    grafica = sns.heatmap(df_MC, annot=True, cbar=False)
    grafica.set(xlabel="Predicion",ylabel="Realidad")
    
    
    # generate a no skill prediction (majority class)
    ns_probs = [0 for _ in range(len(labels_test))]
    # Calculamos las curvas ROC
    ns_fpr, ns_tpr, _ = roc_curve(labels_test, ns_probs)
    lr_fpr, lr_tpr, _ = roc_curve(labels_test, pred)
    # calculate scores
    ns_auc = roc_auc_score(labels_test, ns_probs)
    lr_auc = roc_auc_score(labels_test, pred)    
    # summarize scores
    print('No Skill: ROC AUC=%.3f' % (ns_auc))
    print(str(model.nodos1), ': ROC AUC=%.3f' % (lr_auc))      
    # Pintamos las curvas ROC
    plt.figure(4)
    plt.plot(ns_fpr, ns_tpr, linestyle='--')
    plt.plot(lr_fpr, lr_tpr, marker='.', label=str(model.nodos1))
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()
    
    # Curva Recall
    lr_precision, lr_recall, _ = precision_recall_curve(labels_test, pred)
    print("Precision: ",lr_precision[1])
    print("Recall: ", lr_recall[1])
    lr_f1, lr_auc = f1_score(labels_test, pred), auc(lr_recall, lr_precision)
    print(str(model.nodos1), ': f1=%.3f auc=%.3f' % (lr_f1, lr_auc))
    no_skill = len(labels_test[labels_test==1]) / len(labels_test)    
    plt.figure(5)    
    plt.plot([0, 1], [no_skill, no_skill], linestyle='--')
    plt.plot(lr_recall, lr_precision, marker='.', label=str(model.nodos1))
    plt.xlabel('Recall')
    plt.ylabel('Accuracy')
    plt.legend()       
    
def evaluarmulticlass(model):
    model.eval()
    pred = model(torch.from_numpy(features_test).float())
    pred = torch.max(pred,1)[1]
    pred = pred.data.numpy()
    MC = confusion_matrix(labels_test, pred).ravel()
    print("Matriz de confusion:", MC)
   
        
    # precision recall curve
    listpath=["Benign", "SMS", "Riskware", "Adware", "Banking"]
    df_MC = pd.DataFrame(MC.reshape(5,5), index=listpath, columns=listpath)
    plt.figure(3)
    grafica = sns.heatmap(df_MC, annot=True, cbar=False)
    grafica.set(xlabel="Prediction",ylabel="Reality")
    
    precision = dict()
    recall = dict()
    f1=f1_score(labels_test, pred, average=None)
    labels_tests = label_binarize(labels_test, classes=[0,1,2,3,4])
    preds = label_binarize(pred, classes=[0,1,2,3,4])
    plt.figure(4)
    for i in range(5):
        acc=accuracy_score(labels_tests[:, i], preds[:, i])
        precision[i], recall[i], _ = precision_recall_curve(labels_tests[:, i],
                                                            preds[:, i])
        plt.plot(recall[i], precision[i], lw=2, label=str(listpath[i]))
        
        if len(recall[i]) == 2:
            print("Precision de " + str(listpath[i]) + ": ", precision[i][0])
            print("Recall de " + str(listpath[i]) + ": ", recall[i][0])
        else:
            print("Precision de " + str(listpath[i]) + ": ", precision[i][1])
            print("Recall de " + str(listpath[i]) + ": ", recall[i][1])
        print("AUC P-R de " + str(listpath[i]) + ": ", auc(recall[i], precision[i]))
        print("F1 de " + str(listpath[i]) + ": ", f1[i])
        print("Acc de " + str(listpath[i]) + ": ", 100*acc, "%")
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.legend()
    
    # roc curve
    fpr = dict()
    tpr = dict()
    
    plt.figure(5)
    for i in range(5):
        fpr[i], tpr[i], _ = roc_curve(labels_tests[:, i],
                                      preds[:, i])
        plt.plot(fpr[i], tpr[i], lw=2, label=str(listpath[i]))
        rocauc=roc_auc_score(labels_tests[:, i], preds[:, i], average=None)
        print("ROC AUC de " + str(listpath[i]) + ": ", rocauc)
    
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend()
    plt.show()  
    
# print("Evaluando Sigmoid")
# evaluarbinary(model_Sigmoid)
print("Evaluando PRelU")
evaluarmulticlass(model_RelU)
# print("Evaluando Tanh")
# evaluarmulticlass(model_Tanh)

torch.save(model_RelU.state_dict(), "archivomalware.pt")




